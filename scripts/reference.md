# Reference

## Self-Knowledge Distillation methods:

[1] DDGSD: Data-Distortion Guided Self-Distillation for Deep Neural Networks. AAAI-2019

[2] DKS: Deeply-supervised Knowledge Synergy. CVPR-2019.

[3] SAD: Learning Lightweight Lane Detection CNNs by Self Attention Distillation. ICCV-2019.

[4] BYOT: Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation. ICCV-2019

[5] Tf-KD-reg: Revisiting Knowledge Distillation via Label Smoothing Regularization. CVPR-2020.

[6] CS-KD: Regularizing Class-wise Predictions via Self-knowledge Distillation. CVPR-2020.

[7] FRSKD: Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation. CVPR-2021.

[8] Self-distillation with batch knowledge ensembling improves imagenet classification. arXiv:2104.13298
## Data augmentation and regularization methods:

[1] Label Smoothing: Rethinking the inception architecture for computer vision. CVPR-2016.

[2] Virtual Softmax:  Virtual class enhanced discriminative embedding learning. NeurIPS-2018.

[3] Focal Loss: Focal loss for dense object detection. ICCV-2017. 

[4] Maximum Entropy: Regularizing neural networks by penalizing confident output distributions. ICLR Workshops 2017.

[5] Cutout: Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552

[6] Random Erase: Random erasing data augmentation. AAAI-2020.

[7] Mixup: mixup: Beyond empirical risk minimization. ICLR-2018.

[8] CutMix: Cutmix: Regularization strategy to train strong classifiers with localizable features. ICCV-2019.

[9] Autoaugment: Autoaugment: Learning augmentation strategies from data. CVPR-2019.

[10] Randaugment: Practical automated data augmentation with a reduced search space. CVPR Workshops-2020.

[11] Augmix: A simple data processing method to improve robustness and uncertainty.arXiv:1912.02781.

[12] Trivialaugment: Tuning-free yet state-of-the-art data augmentation. ICCV-2021.

